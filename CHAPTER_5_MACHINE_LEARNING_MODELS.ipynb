{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5facc2ef-6215-4477-9f8c-e4bd44be7110",
   "metadata": {},
   "source": [
    "# CHAPTER 5: MACHINE LEARNING MODELS\n",
    "\n",
    "While **feature and target engineering** make up **80% of the work**, it’s still crucial to properly handle the remaining **20%**. The advantage is that this final step becomes relatively straightforward if you’ve done the previous steps well and understand how your models function. \n",
    "\n",
    "At this stage, we already know our **target** and the **features** available to explain it. The task now is to find the **best method to model this relationship**: this method is the model.\n",
    "\n",
    "*PS: The goal of this chapter is to explain the **strengths and weaknesses** of several models (but you can use others in the same mind), not to evaluate them (that’s for the next chapter).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3170ef-76d6-4bca-aecf-907c7a1667cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611289e0-671e-4dc9-9514-2c255364c404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>tick_volume</th>\n",
       "      <th>high_time</th>\n",
       "      <th>low_time</th>\n",
       "      <th>hurst</th>\n",
       "      <th>0_to_20</th>\n",
       "      <th>20_to_40</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_volatility_yang_zhang</th>\n",
       "      <th>linear_slope_6M</th>\n",
       "      <th>linear_slope_3M</th>\n",
       "      <th>linear_slope_1M</th>\n",
       "      <th>open_close_var</th>\n",
       "      <th>candle_color</th>\n",
       "      <th>next_candle_color</th>\n",
       "      <th>future_market_regime</th>\n",
       "      <th>labeling</th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-11-14 00:00:00</th>\n",
       "      <td>1.24750</td>\n",
       "      <td>1.24789</td>\n",
       "      <td>1.24588</td>\n",
       "      <td>1.24665</td>\n",
       "      <td>14537.0</td>\n",
       "      <td>2014-11-14 00:12:00</td>\n",
       "      <td>2014-11-14 03:09:00</td>\n",
       "      <td>0.606340</td>\n",
       "      <td>12.446352</td>\n",
       "      <td>23.175966</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11.600000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-14 04:00:00</th>\n",
       "      <td>1.24665</td>\n",
       "      <td>1.24669</td>\n",
       "      <td>1.24266</td>\n",
       "      <td>1.24307</td>\n",
       "      <td>17128.0</td>\n",
       "      <td>2014-11-14 04:00:00</td>\n",
       "      <td>2014-11-14 07:46:00</td>\n",
       "      <td>0.710822</td>\n",
       "      <td>12.552301</td>\n",
       "      <td>12.970711</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.283333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-14 08:00:00</th>\n",
       "      <td>1.24306</td>\n",
       "      <td>1.24711</td>\n",
       "      <td>1.24262</td>\n",
       "      <td>1.24623</td>\n",
       "      <td>35033.0</td>\n",
       "      <td>2014-11-14 10:34:00</td>\n",
       "      <td>2014-11-14 08:04:00</td>\n",
       "      <td>0.583402</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00317</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.600000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-14 12:00:00</th>\n",
       "      <td>1.24614</td>\n",
       "      <td>1.24686</td>\n",
       "      <td>1.23982</td>\n",
       "      <td>1.24140</td>\n",
       "      <td>41784.0</td>\n",
       "      <td>2014-11-14 12:02:00</td>\n",
       "      <td>2014-11-14 15:36:00</td>\n",
       "      <td>0.593497</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.283333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-14 16:00:00</th>\n",
       "      <td>1.24140</td>\n",
       "      <td>1.25435</td>\n",
       "      <td>1.24054</td>\n",
       "      <td>1.25140</td>\n",
       "      <td>74087.0</td>\n",
       "      <td>2014-11-14 19:17:00</td>\n",
       "      <td>2014-11-14 16:05:00</td>\n",
       "      <td>0.682967</td>\n",
       "      <td>25.416667</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.583333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-12 20:00:00</th>\n",
       "      <td>1.06066</td>\n",
       "      <td>1.06263</td>\n",
       "      <td>1.06062</td>\n",
       "      <td>1.06227</td>\n",
       "      <td>6069.0</td>\n",
       "      <td>2024-11-12 23:05:00</td>\n",
       "      <td>2024-11-12 20:00:00</td>\n",
       "      <td>0.499964</td>\n",
       "      <td>6.276151</td>\n",
       "      <td>33.472803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.00161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.133333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-13 00:00:00</th>\n",
       "      <td>1.06180</td>\n",
       "      <td>1.06288</td>\n",
       "      <td>1.06106</td>\n",
       "      <td>1.06277</td>\n",
       "      <td>4596.0</td>\n",
       "      <td>2024-11-13 03:46:00</td>\n",
       "      <td>2024-11-13 02:15:00</td>\n",
       "      <td>0.469370</td>\n",
       "      <td>16.595745</td>\n",
       "      <td>32.340426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.00097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.133333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-13 04:00:00</th>\n",
       "      <td>1.06277</td>\n",
       "      <td>1.06290</td>\n",
       "      <td>1.06092</td>\n",
       "      <td>1.06127</td>\n",
       "      <td>3868.0</td>\n",
       "      <td>2024-11-13 04:00:00</td>\n",
       "      <td>2024-11-13 07:22:00</td>\n",
       "      <td>0.553327</td>\n",
       "      <td>7.916667</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.00150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.133333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-13 08:00:00</th>\n",
       "      <td>1.06127</td>\n",
       "      <td>1.06295</td>\n",
       "      <td>1.05931</td>\n",
       "      <td>1.06266</td>\n",
       "      <td>7587.0</td>\n",
       "      <td>2024-11-13 11:55:00</td>\n",
       "      <td>2024-11-13 10:09:00</td>\n",
       "      <td>0.541830</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.133333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-13 12:00:00</th>\n",
       "      <td>1.06265</td>\n",
       "      <td>1.06539</td>\n",
       "      <td>1.06097</td>\n",
       "      <td>1.06362</td>\n",
       "      <td>12339.0</td>\n",
       "      <td>2024-11-13 15:30:00</td>\n",
       "      <td>2024-11-13 13:31:00</td>\n",
       "      <td>0.549468</td>\n",
       "      <td>31.250000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>0.00097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.133333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15538 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open     high      low    close  tick_volume  \\\n",
       "time                                                                   \n",
       "2014-11-14 00:00:00  1.24750  1.24789  1.24588  1.24665      14537.0   \n",
       "2014-11-14 04:00:00  1.24665  1.24669  1.24266  1.24307      17128.0   \n",
       "2014-11-14 08:00:00  1.24306  1.24711  1.24262  1.24623      35033.0   \n",
       "2014-11-14 12:00:00  1.24614  1.24686  1.23982  1.24140      41784.0   \n",
       "2014-11-14 16:00:00  1.24140  1.25435  1.24054  1.25140      74087.0   \n",
       "...                      ...      ...      ...      ...          ...   \n",
       "2024-11-12 20:00:00  1.06066  1.06263  1.06062  1.06227       6069.0   \n",
       "2024-11-13 00:00:00  1.06180  1.06288  1.06106  1.06277       4596.0   \n",
       "2024-11-13 04:00:00  1.06277  1.06290  1.06092  1.06127       3868.0   \n",
       "2024-11-13 08:00:00  1.06127  1.06295  1.05931  1.06266       7587.0   \n",
       "2024-11-13 12:00:00  1.06265  1.06539  1.06097  1.06362      12339.0   \n",
       "\n",
       "                               high_time             low_time     hurst  \\\n",
       "time                                                                      \n",
       "2014-11-14 00:00:00  2014-11-14 00:12:00  2014-11-14 03:09:00  0.606340   \n",
       "2014-11-14 04:00:00  2014-11-14 04:00:00  2014-11-14 07:46:00  0.710822   \n",
       "2014-11-14 08:00:00  2014-11-14 10:34:00  2014-11-14 08:04:00  0.583402   \n",
       "2014-11-14 12:00:00  2014-11-14 12:02:00  2014-11-14 15:36:00  0.593497   \n",
       "2014-11-14 16:00:00  2014-11-14 19:17:00  2014-11-14 16:05:00  0.682967   \n",
       "...                                  ...                  ...       ...   \n",
       "2024-11-12 20:00:00  2024-11-12 23:05:00  2024-11-12 20:00:00  0.499964   \n",
       "2024-11-13 00:00:00  2024-11-13 03:46:00  2024-11-13 02:15:00  0.469370   \n",
       "2024-11-13 04:00:00  2024-11-13 04:00:00  2024-11-13 07:22:00  0.553327   \n",
       "2024-11-13 08:00:00  2024-11-13 11:55:00  2024-11-13 10:09:00  0.541830   \n",
       "2024-11-13 12:00:00  2024-11-13 15:30:00  2024-11-13 13:31:00  0.549468   \n",
       "\n",
       "                       0_to_20   20_to_40  ...  rolling_volatility_yang_zhang  \\\n",
       "time                                       ...                                  \n",
       "2014-11-14 00:00:00  12.446352  23.175966  ...                            NaN   \n",
       "2014-11-14 04:00:00  12.552301  12.970711  ...                            NaN   \n",
       "2014-11-14 08:00:00  22.500000  15.000000  ...                            NaN   \n",
       "2014-11-14 12:00:00   5.000000   7.500000  ...                            NaN   \n",
       "2014-11-14 16:00:00  25.416667  11.250000  ...                            NaN   \n",
       "...                        ...        ...  ...                            ...   \n",
       "2024-11-12 20:00:00   6.276151  33.472803  ...                       0.002094   \n",
       "2024-11-13 00:00:00  16.595745  32.340426  ...                       0.002097   \n",
       "2024-11-13 04:00:00   7.916667  15.000000  ...                       0.002015   \n",
       "2024-11-13 08:00:00   3.750000  33.333333  ...                       0.001949   \n",
       "2024-11-13 12:00:00  31.250000  25.000000  ...                       0.002024   \n",
       "\n",
       "                     linear_slope_6M  linear_slope_3M  linear_slope_1M  \\\n",
       "time                                                                     \n",
       "2014-11-14 00:00:00              NaN              NaN              NaN   \n",
       "2014-11-14 04:00:00              NaN              NaN              NaN   \n",
       "2014-11-14 08:00:00              NaN              NaN              NaN   \n",
       "2014-11-14 12:00:00              NaN              NaN              NaN   \n",
       "2014-11-14 16:00:00              NaN              NaN              NaN   \n",
       "...                              ...              ...              ...   \n",
       "2024-11-12 20:00:00         0.000022        -0.000122        -0.000109   \n",
       "2024-11-13 00:00:00         0.000022        -0.000123        -0.000113   \n",
       "2024-11-13 04:00:00         0.000021        -0.000124        -0.000117   \n",
       "2024-11-13 08:00:00         0.000021        -0.000125        -0.000122   \n",
       "2024-11-13 12:00:00         0.000020        -0.000126        -0.000125   \n",
       "\n",
       "                     open_close_var  candle_color  next_candle_color  \\\n",
       "time                                                                   \n",
       "2014-11-14 00:00:00        -0.00085           0.0                0.0   \n",
       "2014-11-14 04:00:00        -0.00358           0.0                1.0   \n",
       "2014-11-14 08:00:00         0.00317           1.0                0.0   \n",
       "2014-11-14 12:00:00        -0.00474           0.0                1.0   \n",
       "2014-11-14 16:00:00         0.01000           1.0                1.0   \n",
       "...                             ...           ...                ...   \n",
       "2024-11-12 20:00:00         0.00161           1.0                1.0   \n",
       "2024-11-13 00:00:00         0.00097           1.0                0.0   \n",
       "2024-11-13 04:00:00        -0.00150           0.0                1.0   \n",
       "2024-11-13 08:00:00         0.00139           1.0                1.0   \n",
       "2024-11-13 12:00:00         0.00097           1.0                0.0   \n",
       "\n",
       "                     future_market_regime   labeling  dummy  \n",
       "time                                                         \n",
       "2014-11-14 00:00:00                   NaN -11.600000      0  \n",
       "2014-11-14 04:00:00                   NaN  11.283333      1  \n",
       "2014-11-14 08:00:00                   NaN  -3.600000      0  \n",
       "2014-11-14 12:00:00                   NaN   3.283333      1  \n",
       "2014-11-14 16:00:00                   NaN  56.583333      1  \n",
       "...                                   ...        ...    ...  \n",
       "2024-11-12 20:00:00                   0.0 -17.133333      0  \n",
       "2024-11-13 00:00:00                   0.0 -13.133333      0  \n",
       "2024-11-13 04:00:00                   0.0  -9.133333      0  \n",
       "2024-11-13 08:00:00                   0.0  -5.133333      0  \n",
       "2024-11-13 12:00:00                   0.0  -1.133333      0  \n",
       "\n",
       "[15538 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dataset containing the features and the signals (already correctly shifted) \n",
    "df = pd.read_parquet(\"DATA/EURUSD_4H_dataset_signal_included.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467be27e-9350-4c0c-8682-d8e56e200944",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_X = ['hurst', 'market_regime', 'kama_diff', 'autocorr_20', 'autocorr_50', 'ret_log_10',\n",
    "       'rolling_volatility_yang_zhang', 'linear_slope_6M', 'linear_slope_3M']\n",
    "col_y = \"dummy\"\n",
    "\n",
    "# Remove the Nan values\n",
    "df_clean = df[list_X + [col_y]].dropna()\n",
    "\n",
    "\n",
    "# Split our data into features and target\n",
    "X_train = df_clean.iloc[0:3_000,:][list_X]\n",
    "y_train = df_clean.iloc[0:3_000,:][col_y]\n",
    "\n",
    "X_test = df_clean.iloc[3_000:4_000,:][list_X]\n",
    "y_test = df_clean.iloc[3_000:4_000,:][col_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13763a9-83cf-488a-806c-0f575a838868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test) # WE DO NOT FIT THE MODEL ON THE TEST DATA ONLY ON THE TRAIN DATA\n",
    "\n",
    "X_train_sc_df = pd.DataFrame(X_train_sc, columns=X_train.columns)\n",
    "X_test_sc_df = pd.DataFrame(X_test_sc, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd758ebf-eadf-4c8e-bb1b-c4d17f71efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def model_output_overview(model_class, X_train_sc=X_train_sc, X_test_sc=X_test_sc, y_train=y_train, y_test=y_test):\n",
    "    model = model_class.fit(X_train_sc, y_train.values)\n",
    "    y_pred = model.predict(X_test_sc)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    precision_class_0 = 100 * conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[1][0])\n",
    "    precision_class_1 = 100 * conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[0][1])\n",
    "\n",
    "    print(f\"Precision Class 0: {precision_class_0:.2f} % \\t Precision Class 1: {precision_class_1:.2f} %\")\n",
    "\n",
    "    print(f\"NB Prediction Class 0: {(conf_matrix[0][0] + conf_matrix[1][0])} \\t NB Prediction Class 1: {(conf_matrix[1][1] + conf_matrix[0][1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3c3ad-9408-4aef-a18c-63a418440b78",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 5.1. LINEAR MODELS\n",
    "\n",
    "In supervised machine learning, there are two types of models: **regression and classification**. Here, since our target is a dummy variable (0 or 1), we will use classification models. However, all the explanations and tips provided for each model will also apply to regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296e67c-0784-4261-8319-3314c15736c7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### 5.1.1. Linear/Logistic Regression\n",
    "\n",
    "**Linear and logistic regression** are linear models designed to capture linear relationships in the data. In trading, purely linear relationships are rare, and these models often struggle to handle the complexity of market dynamics. However, **\"not enough\" does not mean \"useless\"**! These models are extremely fast to train, making them valuable tools for quickly assessing the level of linear dependence between features and the target. They can provide valuable insights into your data's structure and serve as a baseline for more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db53b44-b810-492f-b1f3-e58677b3841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Class 0: 33.93 % \t Precision Class 1: 57.57 %\n",
      "NB Prediction Class 0: 392 \t NB Prediction Class 1: 608\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "model_output_overview(LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7bb59-cea7-4d31-a647-c3819a6e5afd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### 5.1.2. Linear SVM\n",
    "\n",
    "Linear SVMs are **robust linear models** that find the **optimal hyperplane** for separating data, performing well even with **noisy data** and **small datasets**. However, their **training time can increase significantly on larger datasets** due to the cost of finding support vectors. When **linear separability** is suspected, they offer **strong performance** but require **standardized data** due to their geometrical basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed5b041-6634-4fb9-ab14-81532807bb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Class 0: 33.93 % \t Precision Class 1: 57.57 %\n",
      "NB Prediction Class 0: 392 \t NB Prediction Class 1: 608\n"
     ]
    }
   ],
   "source": [
    "# LINEAR SVC\n",
    "from sklearn.svm import LinearSVC, LinearSVR\n",
    "model_output_overview(LinearSVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c03f5ab-7ce1-4511-b16c-fee757d43589",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 5.2. NON-LINEAR MODELS\n",
    "\n",
    "The second family of models is the **non-linear models**, which capture **non-linear relationships** (quite obvious, I know). These are the **most used models in finance and trading** because **most of the information is non-linear**. In other words, there is a lot of **valuable information** that only these models can detect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2d7b2-7d3f-4930-8791-6d9c310b5900",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### 5.2.1. Non-Linear SVM\n",
    "\n",
    "Non-linear SVMs share the **same strengths and weaknesses as Linear SVMs**, with an **even greater sensitivity to large dataset issues**. The key difference is that they use a **non-linear kernel** to find the **optimal hyperplane** for separating data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe0b95e-cbde-410a-8645-3a6845c52a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Class 0: 39.63 % \t Precision Class 1: 61.89 %\n",
      "NB Prediction Class 0: 651 \t NB Prediction Class 1: 349\n"
     ]
    }
   ],
   "source": [
    "# NON LINEAR SVC\n",
    "from sklearn.svm import SVC, SVR\n",
    "model_output_overview(SVC(C=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a821d5b-0a2c-44cd-8909-9ff9127c9fff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### 5.2.2. Random Forest\n",
    "\n",
    "Random Forests are **ensemble models** that build **multiple decision trees** and aggregate their predictions, making them **robust to overfitting** and effective at handling **non-linear relationships**. They work well with **categorical features**, including **dummy variables**, because they split data based on thresholds rather than relying on linear transformations (**and yes, dummy variables are indeed effective with Random Forests**). Additionally, they are **less sensitive to scaling** and can handle **missing data** to some extent, but they can become **computationally expensive on very large datasets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0644771a-8426-4875-864b-227f76b5ee69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Class 0: 40.76 % \t Precision Class 1: 68.31 %\n",
      "NB Prediction Class 0: 817 \t NB Prediction Class 1: 183\n"
     ]
    }
   ],
   "source": [
    "# STANDARD RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "model_output_overview(RandomForestClassifier(n_estimators=1_000, max_depth=100, random_state=56))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639d211-01ba-43d0-9682-2a0e694919a5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### 5.2.3. Extra Trees\n",
    "\n",
    "Extra Trees (Extremely Randomized Trees) are **ensemble models** similar to Random Forests but differ by introducing **additional randomness** during tree construction. They split nodes using **random thresholds**, making them **faster to train** and often **less prone to overfitting** in certain cases. Like Random Forests, they handle **non-linear relationships** well and work effectively with **dummy variables** since they use **threshold-based splits**. They are also **robust to scaling and noise** but may require **tuning to balance bias and variance** for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42b34d1-ff5d-420f-9c9f-bb03d9a303b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Class 0: 41.35 % \t Precision Class 1: 69.16 %\n",
      "NB Prediction Class 0: 786 \t NB Prediction Class 1: 214\n"
     ]
    }
   ],
   "source": [
    "# Extra Tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegression\n",
    "model_class = ExtraTreesClassifier(n_estimators=1_000, max_depth=None, min_samples_split=2, random_state=56)\n",
    "model_output_overview(model_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb1b7d-6696-4630-876e-53c0d7bd4d1f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### 5.2.4. Bagging\n",
    "\n",
    "Bagging (Bootstrap Aggregating) combines **predictions from multiple models** trained on **bootstrapped subsets** of the data to **reduce variance** and **improve stability**. It works well with **high-variance models** like decision trees, enhancing **robustness without increasing bias**. However, it can be **computationally expensive** due to the need for multiple model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3cc318d-2d31-40b7-93ee-339de5eacf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Class 0: 40.00 % \t Precision Class 1: 62.40 %\n",
      "NB Prediction Class 0: 625 \t NB Prediction Class 1: 375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "model_output_overview(BaggingClassifier(estimator=SVC(C=3),\n",
    "                        n_estimators=10, random_state=56))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990afad3-80fa-47f8-b5bf-be7ca522c950",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### 5.2.5. Neural Networks\n",
    "\n",
    "The **MLPClassifier** is a **neural network model** capable of capturing **complex, non-linear relationships**. It supports **multiple hidden layers** and **backpropagation**, making it **versatile** for various tasks. However, it requires **careful tuning** (e.g., hidden layers, activation functions) and **sufficient data**, as it is prone to **overfitting** and **sensitive to scaling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d9b7d92-94b6-4f91-b698-08a92989b9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Class 0: 40.13 % \t Precision Class 1: 64.35 %\n",
      "NB Prediction Class 0: 770 \t NB Prediction Class 1: 230\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "model_class = MLPClassifier(solver='lbfgs',\n",
    "                    hidden_layer_sizes=(100, 20, 10), random_state=56)\n",
    "model_output_overview(model_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce9904-08ed-489c-83fb-fa4b00d69016",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 5.3. Ensemble Methods\n",
    "\n",
    "In the previous section, we explored several ensemble methods (Random Forest, Bagging, Extra Trees) that use the same model multiple times. Here, we introduce a **voting method** that combines **different models** (both linear and non-linear) to create a single model that, ideally, delivers the **best performance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a3973bf-ee37-4cce-84a6-359fc5f50703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Class 0: 40.17 % \t Precision Class 1: 63.67 %\n",
      "NB Prediction Class 0: 722 \t NB Prediction Class 1: 278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "\n",
    "model_class = VotingClassifier(\n",
    "    estimators=[('lr', LogisticRegression()),\n",
    "                ('svc', SVC(C=3)),\n",
    "                ('rfc', RandomForestClassifier(n_estimators=1_000, max_depth=100, random_state=56)),\n",
    "               ('ext', ExtraTreesClassifier(n_estimators=1_000, max_depth=None, min_samples_split=2, random_state=56)),\n",
    "               ('bagsvc',BaggingClassifier(estimator=SVC(C=3),\n",
    "                        n_estimators=10, random_state=56)),\n",
    "               ('dnn', MLPClassifier(solver='adam', alpha=3.16e-5,\n",
    "                    hidden_layer_sizes=(100, 20, 10), random_state=56))],\n",
    "    voting='hard')\n",
    "\n",
    "model_output_overview(model_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a21b2c73-558f-44e8-822c-a60f90d07147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.889999999999997"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Very quick overview about the profit (only when it is possible which is not always the case)\n",
    "precision = 0.63\n",
    "nb_trade = 199\n",
    "(precision * 0.0048 - 0.0052 * (1-precision)) * nb_trade * 100 # in 8 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ee85e-3ccd-4be6-a70a-f085b7773ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
